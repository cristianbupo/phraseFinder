[
  {
    "text": "- [Announcer] This Big\nThink interview segment",
    "start": 0.45,
    "duration": 1.35
  },
  {
    "text": "is brought you by Ground News.",
    "start": 1.8,
    "duration": 2.13
  },
  {
    "text": "Where seeing all sides of a story matters.",
    "start": 3.93,
    "duration": 2.46
  },
  {
    "text": "Compare coverage, spot bias,",
    "start": 6.39,
    "duration": 1.98
  },
  {
    "text": "and see who's behind every headline.",
    "start": 8.37,
    "duration": 2.1
  },
  {
    "text": "Visit ground.news/bigthink",
    "start": 10.47,
    "duration": 1.89
  },
  {
    "text": "for 40% off unlimited access\nwith the Vantage Plan.",
    "start": 12.36,
    "duration": 3.393
  },
  {
    "text": "- What would a totalitarian society",
    "start": 16.95,
    "duration": 4.963
  },
  {
    "text": "that is not well disposed to our own",
    "start": 21.913,
    "duration": 4.157
  },
  {
    "text": "do if it had God-like power?",
    "start": 26.07,
    "duration": 3.297
  },
  {
    "text": "And we don't wanna find out.",
    "start": 29.367,
    "duration": 2.733
  },
  {
    "text": "That said, I think the\nreal solution ultimately",
    "start": 32.1,
    "duration": 4.14
  },
  {
    "text": "is to achieve a world that\nis politically sane enough",
    "start": 36.24,
    "duration": 4.26
  },
  {
    "text": "that offers enough of\na basis for cooperation",
    "start": 40.5,
    "duration": 2.82
  },
  {
    "text": "at a global scale such that we can get out",
    "start": 43.32,
    "duration": 2.97
  },
  {
    "text": "of this arms race condition.",
    "start": 46.29,
    "duration": 2.55
  },
  {
    "text": "You know, we wanna live in\na world where, you know,",
    "start": 48.84,
    "duration": 2.22
  },
  {
    "text": "we don't fear the Chinese\nand they don't fear us.",
    "start": 51.06,
    "duration": 5.0
  },
  {
    "text": "And we don't live in a\nworld where at least one",
    "start": 56.07,
    "duration": 2.49
  },
  {
    "text": "of those parties is right to\nbe feeling fear at this moment",
    "start": 58.56,
    "duration": 3.6
  },
  {
    "text": "because the other party\nreally is committed to goals",
    "start": 62.16,
    "duration": 2.52
  },
  {
    "text": "that are inimical to\neverything that we value",
    "start": 64.68,
    "duration": 3.45
  },
  {
    "text": "and are right to value.",
    "start": 68.13,
    "duration": 1.56
  },
  {
    "text": "We should get powerful\nAI before those people.",
    "start": 69.69,
    "duration": 3.833
  },
  {
    "text": "- [Narrator] How can we develop",
    "start": 75.15,
    "duration": 0.93
  },
  {
    "text": "artificial intelligence responsibly?",
    "start": 76.08,
    "duration": 3.0
  },
  {
    "text": "- Well, we can mean at least two things",
    "start": 81.42,
    "duration": 2.07
  },
  {
    "text": "by artificial intelligence.",
    "start": 83.49,
    "duration": 1.2
  },
  {
    "text": "There's the narrow version where we have",
    "start": 84.69,
    "duration": 3.67
  },
  {
    "text": "increasingly competent machines.",
    "start": 89.25,
    "duration": 2.07
  },
  {
    "text": "I mean, now we're experiencing\nlarge language models",
    "start": 91.32,
    "duration": 2.13
  },
  {
    "text": "that are quite amazing in\ntheir ability to produce text.",
    "start": 93.45,
    "duration": 3.87
  },
  {
    "text": "And then there's the wider\nversion, often called AGI,",
    "start": 97.32,
    "duration": 3.3
  },
  {
    "text": "or artificial general intelligence,",
    "start": 100.62,
    "duration": 2.43
  },
  {
    "text": "which refers to a\nhuman-like capacity to do",
    "start": 103.05,
    "duration": 4.88
  },
  {
    "text": "many different types of things well,",
    "start": 109.47,
    "duration": 3.36
  },
  {
    "text": "wherein there there's\nno real specialization",
    "start": 112.83,
    "duration": 2.82
  },
  {
    "text": "and there's no degradation of function",
    "start": 115.65,
    "duration": 2.64
  },
  {
    "text": "across those domains, right?",
    "start": 118.29,
    "duration": 1.98
  },
  {
    "text": "So the better you get at parsing language,",
    "start": 120.27,
    "duration": 2.73
  },
  {
    "text": "that doesn't mean you get\nworse at solving math problems",
    "start": 123.0,
    "duration": 3.45
  },
  {
    "text": "or recognizing faces.",
    "start": 126.45,
    "duration": 1.89
  },
  {
    "text": "So the moment we get something\nthat is truly general,",
    "start": 128.34,
    "duration": 3.422
  },
  {
    "text": "that is human-like in its\nability to solve problems",
    "start": 131.762,
    "duration": 4.678
  },
  {
    "text": "across a range of environments\nand with no degradation",
    "start": 136.44,
    "duration": 4.154
  },
  {
    "text": "and it's learning, it just\nbecomes, you know, in the end,",
    "start": 140.594,
    "duration": 4.786
  },
  {
    "text": "self-improving, one thing\nbecomes obvious, right?",
    "start": 145.38,
    "duration": 3.18
  },
  {
    "text": "First, this thing will\nbe immediately superhuman",
    "start": 148.56,
    "duration": 3.24
  },
  {
    "text": "because we will not have built",
    "start": 151.8,
    "duration": 1.53
  },
  {
    "text": "any of these individual capacities",
    "start": 153.33,
    "duration": 1.8
  },
  {
    "text": "at a level lower than human, right?",
    "start": 155.13,
    "duration": 1.83
  },
  {
    "text": "So, you know, your\ncalculator in your phone",
    "start": 156.96,
    "duration": 2.67
  },
  {
    "text": "is already superhuman for arithmetic.",
    "start": 159.63,
    "duration": 2.7
  },
  {
    "text": "There's no way the calculator\nwe put into the AGI",
    "start": 162.33,
    "duration": 3.93
  },
  {
    "text": "is gonna be worse than the one\nwe put in your phone, right?",
    "start": 166.26,
    "duration": 2.19
  },
  {
    "text": "So the moment we get this\nomnibus suite of capacities",
    "start": 168.45,
    "duration": 5.0
  },
  {
    "text": "that are truly general,\nwe have to recognize that",
    "start": 174.3,
    "duration": 2.79
  },
  {
    "text": "that human level intelligence is a mirage",
    "start": 177.09,
    "duration": 3.12
  },
  {
    "text": "that we never even, you know, arrived at",
    "start": 180.21,
    "duration": 2.73
  },
  {
    "text": "for even a moment, right?",
    "start": 182.94,
    "duration": 2.55
  },
  {
    "text": "We just crossed over from this piecemeal",
    "start": 185.49,
    "duration": 3.12
  },
  {
    "text": "kinda superhuman narrow\nversions of intelligence.",
    "start": 188.61,
    "duration": 3.48
  },
  {
    "text": "You have a superhuman calculator,",
    "start": 192.09,
    "duration": 1.77
  },
  {
    "text": "and you now have a superhuman\nlarge language model.",
    "start": 193.86,
    "duration": 3.57
  },
  {
    "text": "When all this gets knit together\nby whatever architecture,",
    "start": 197.43,
    "duration": 3.75
  },
  {
    "text": "you will suddenly be in the presence",
    "start": 201.18,
    "duration": 1.62
  },
  {
    "text": "of the most competent mind\nyou've ever met, right?",
    "start": 202.8,
    "duration": 4.317
  },
  {
    "text": "And I think that the thing\nthat's important to recognize",
    "start": 207.117,
    "duration": 2.553
  },
  {
    "text": "there is that if we're truly talking about",
    "start": 209.67,
    "duration": 2.49
  },
  {
    "text": "general intelligence, we're\ntalking about autonomy,",
    "start": 212.16,
    "duration": 3.6
  },
  {
    "text": "we're talking about a\nrelationship, therefore,",
    "start": 215.76,
    "duration": 3.27
  },
  {
    "text": "we're talking about being in\nthe presence of another mind,",
    "start": 219.03,
    "duration": 4.92
  },
  {
    "text": "whether it's conscious or not,",
    "start": 223.95,
    "duration": 1.29
  },
  {
    "text": "we can leave consciousness aside",
    "start": 225.24,
    "duration": 1.38
  },
  {
    "text": "because I think it's genuinely uncertain",
    "start": 226.62,
    "duration": 1.98
  },
  {
    "text": "whether a consciousness\ncomes along for the ride",
    "start": 228.6,
    "duration": 3.63
  },
  {
    "text": "as you scale up in intelligence.",
    "start": 232.23,
    "duration": 2.28
  },
  {
    "text": "I happen to think there's\nno reason to expect that",
    "start": 234.51,
    "duration": 2.52
  },
  {
    "text": "at this point.",
    "start": 237.03,
    "duration": 0.833
  },
  {
    "text": "So whether the lights are\non or not in our robots",
    "start": 237.863,
    "duration": 2.527
  },
  {
    "text": "or in our most powerful computers,",
    "start": 240.39,
    "duration": 3.06
  },
  {
    "text": "they'll certainly seem to be conscious",
    "start": 243.45,
    "duration": 2.46
  },
  {
    "text": "because we'll build them that way,",
    "start": 245.91,
    "duration": 1.14
  },
  {
    "text": "or certainly we'll build\nsome of them that way.",
    "start": 247.05,
    "duration": 2.37
  },
  {
    "text": "And we might just lose\nsight of the problem",
    "start": 249.42,
    "duration": 1.71
  },
  {
    "text": "as to whether it's interesting\nintellectually or ethically",
    "start": 251.13,
    "duration": 3.33
  },
  {
    "text": "to figure out whether these\nsystems are conscious,",
    "start": 254.46,
    "duration": 1.89
  },
  {
    "text": "because they're just\ngoing to seem conscious.",
    "start": 256.35,
    "duration": 2.13
  },
  {
    "text": "I mean, certainly if we\nbuild humanoid robots",
    "start": 258.48,
    "duration": 2.4
  },
  {
    "text": "that are more intelligent than we are,",
    "start": 260.88,
    "duration": 2.61
  },
  {
    "text": "we will feel that we're in relationship",
    "start": 263.49,
    "duration": 2.01
  },
  {
    "text": "to conscious entities, and\nmany things follow from that.",
    "start": 265.5,
    "duration": 4.5
  },
  {
    "text": "So there are really two\nlevels of risk here.",
    "start": 270.0,
    "duration": 2.07
  },
  {
    "text": "There's the risk that that bad people,",
    "start": 272.07,
    "duration": 3.45
  },
  {
    "text": "or, you know, badly intentioned people",
    "start": 275.52,
    "duration": 2.79
  },
  {
    "text": "or unwise people will do\nbad things with their AI,",
    "start": 278.31,
    "duration": 5.0
  },
  {
    "text": "whether it's, you know, narrow\nAI of increasing strength",
    "start": 283.53,
    "duration": 2.67
  },
  {
    "text": "or the general AI that they build.",
    "start": 286.2,
    "duration": 2.37
  },
  {
    "text": "And that's one bad outcome that we can try",
    "start": 288.57,
    "duration": 3.27
  },
  {
    "text": "to safeguard against, and that's not easy.",
    "start": 291.84,
    "duration": 2.1
  },
  {
    "text": "But there's this additional problem",
    "start": 295.14,
    "duration": 2.67
  },
  {
    "text": "and probably deeper problem that,",
    "start": 297.81,
    "duration": 2.25
  },
  {
    "text": "in the presence of truly autonomous AGI,",
    "start": 300.06,
    "duration": 4.2
  },
  {
    "text": "general intelligence that is super human,",
    "start": 304.26,
    "duration": 3.45
  },
  {
    "text": "we need not merely worry\nabout human bad actors.",
    "start": 307.71,
    "duration": 3.06
  },
  {
    "text": "We need to worry about what's\ncalled the alignment problem.",
    "start": 310.77,
    "duration": 3.93
  },
  {
    "text": "You know, whether this\nnow more competent mind",
    "start": 314.7,
    "duration": 4.47
  },
  {
    "text": "is aligned with our\ninterests or disposed to be,",
    "start": 319.17,
    "duration": 3.72
  },
  {
    "text": "you know, realigned with our interests",
    "start": 322.89,
    "duration": 1.59
  },
  {
    "text": "whenever we detect that\nthere's some daylight",
    "start": 324.48,
    "duration": 1.74
  },
  {
    "text": "between what it's doing\nand what we want, right?",
    "start": 326.22,
    "duration": 4.32
  },
  {
    "text": "I mean, it is just the kind of\nmind that cares what we want.",
    "start": 330.54,
    "duration": 4.08
  },
  {
    "text": "Have we built it in such a way",
    "start": 334.62,
    "duration": 3.6
  },
  {
    "text": "that it could ever lose sight\nof what is good for us, right?",
    "start": 338.22,
    "duration": 4.53
  },
  {
    "text": "And it's not obvious\nin advance all the ways",
    "start": 342.75,
    "duration": 3.78
  },
  {
    "text": "in which a mines more\nintelligent than our own",
    "start": 346.53,
    "duration": 4.23
  },
  {
    "text": "could grow unaligned with our interests",
    "start": 350.76,
    "duration": 3.72
  },
  {
    "text": "and depart from this ongoing effort",
    "start": 354.48,
    "duration": 4.29
  },
  {
    "text": "to make the world better\nand better for us, right?",
    "start": 358.77,
    "duration": 3.48
  },
  {
    "text": "I mean, just imagine by analogy",
    "start": 362.25,
    "duration": 4.29
  },
  {
    "text": "what it's like for every\nother species on Earth",
    "start": 366.54,
    "duration": 3.0
  },
  {
    "text": "watching humans grow more\nand more powerful, right?",
    "start": 369.54,
    "duration": 3.84
  },
  {
    "text": "Human culture, human\nsociety, human technology.",
    "start": 373.38,
    "duration": 3.21
  },
  {
    "text": "You know, at one point\nwe were hairless apes",
    "start": 376.59,
    "duration": 5.0
  },
  {
    "text": "with sticks and rocks,\nyou know, and flint tools.",
    "start": 381.81,
    "duration": 4.89
  },
  {
    "text": "And that gave us this\noverwhelming advantage, right?",
    "start": 386.7,
    "duration": 4.68
  },
  {
    "text": "Just our ability to\ncooperate through language",
    "start": 391.38,
    "duration": 2.31
  },
  {
    "text": "and the most primitive\ntechnology we began to leverage,",
    "start": 393.69,
    "duration": 5.0
  },
  {
    "text": "already there we were unstoppable",
    "start": 400.38,
    "duration": 3.42
  },
  {
    "text": "when you consider the\ncareer of any other species,",
    "start": 403.8,
    "duration": 3.48
  },
  {
    "text": "even our closest cousins,\nyou know, the Neanderthals,",
    "start": 407.28,
    "duration": 2.67
  },
  {
    "text": "which we very likely wiped out.",
    "start": 409.95,
    "duration": 2.7
  },
  {
    "text": "What will it be like to be\nin the presence of minds",
    "start": 412.65,
    "duration": 4.92
  },
  {
    "text": "that, again, whether\nthey're conscious or not,",
    "start": 417.57,
    "duration": 3.27
  },
  {
    "text": "are so much more competent than we are",
    "start": 420.84,
    "duration": 2.61
  },
  {
    "text": "and so busy doing things\nthat they are forming",
    "start": 423.45,
    "duration": 4.42
  },
  {
    "text": "instrumental goals that we can't\npossibly understand, right?",
    "start": 429.9,
    "duration": 4.92
  },
  {
    "text": "And they're doing all of\nthis so quickly, right?",
    "start": 434.82,
    "duration": 3.81
  },
  {
    "text": "I mean, just speed on its own",
    "start": 438.63,
    "duration": 2.7
  },
  {
    "text": "could be totally destabilizing.",
    "start": 441.33,
    "duration": 1.5
  },
  {
    "text": "I mean, just imagine we\nbuilt AI that was no smarter",
    "start": 442.83,
    "duration": 4.05
  },
  {
    "text": "than the 10 smartest\npeople together in a room,",
    "start": 446.88,
    "duration": 3.18
  },
  {
    "text": "but it just worked a\nbillion times faster, right?",
    "start": 450.06,
    "duration": 3.39
  },
  {
    "text": "Well, what would it be\nlike to be in relationship",
    "start": 453.45,
    "duration": 2.22
  },
  {
    "text": "to 10 people who every time, you know,",
    "start": 455.67,
    "duration": 3.953
  },
  {
    "text": "you stop to think for a second,",
    "start": 459.623,
    "duration": 4.447
  },
  {
    "text": "they did 32 years, you\nknow, a billion seconds",
    "start": 464.07,
    "duration": 5.0
  },
  {
    "text": "worth of cognitive work, right?",
    "start": 469.86,
    "duration": 2.55
  },
  {
    "text": "What would that conversation be like?",
    "start": 472.41,
    "duration": 2.61
  },
  {
    "text": "I mean, it's unimaginable, right?",
    "start": 475.02,
    "duration": 2.01
  },
  {
    "text": "And that's why this\nwhole thing is described",
    "start": 477.03,
    "duration": 3.0
  },
  {
    "text": "as a singularity, right, or\nan intelligence explosion.",
    "start": 480.03,
    "duration": 4.05
  },
  {
    "text": "And the moment this\nreally becomes a concern,",
    "start": 484.08,
    "duration": 3.51
  },
  {
    "text": "once we imagine that\nthe machines themselves",
    "start": 487.59,
    "duration": 3.03
  },
  {
    "text": "could become recursively self-improving,",
    "start": 490.62,
    "duration": 2.82
  },
  {
    "text": "that they could be the agents of improving",
    "start": 493.44,
    "duration": 2.88
  },
  {
    "text": "their own software or they\ncould build the next generation",
    "start": 496.32,
    "duration": 3.99
  },
  {
    "text": "of more competent machines, right?",
    "start": 500.31,
    "duration": 1.83
  },
  {
    "text": "The moment we engineer anything like that,",
    "start": 502.14,
    "duration": 2.55
  },
  {
    "text": "and there's no reason to\nthink we won't do that",
    "start": 504.69,
    "duration": 2.64
  },
  {
    "text": "at this point, something like\nan intelligence explosion",
    "start": 507.33,
    "duration": 3.06
  },
  {
    "text": "certainly becomes conceivable.",
    "start": 510.39,
    "duration": 1.44
  },
  {
    "text": "And whether it happens slowly or quickly,",
    "start": 511.83,
    "duration": 3.54
  },
  {
    "text": "again, I think the thing to\nrecognize is that we have",
    "start": 515.37,
    "duration": 2.85
  },
  {
    "text": "to understand that we will be\nin relationship to other minds",
    "start": 518.22,
    "duration": 4.71
  },
  {
    "text": "more powerful than our own, right?",
    "start": 522.93,
    "duration": 2.16
  },
  {
    "text": "And unless they have been built",
    "start": 525.09,
    "duration": 2.04
  },
  {
    "text": "so as to have, at bottom, a core concern",
    "start": 527.13,
    "duration": 5.0
  },
  {
    "text": "to more faithfully\napproximate what we want,",
    "start": 533.61,
    "duration": 4.77
  },
  {
    "text": "you know, ad infinitum,\nit remains a genuine fear",
    "start": 538.38,
    "duration": 3.81
  },
  {
    "text": "that we could build something that could,",
    "start": 542.19,
    "duration": 3.24
  },
  {
    "text": "at a certain point, no\nlonger care what we want.",
    "start": 545.43,
    "duration": 2.463
  },
  {
    "text": "The first thing to recognize is that",
    "start": 548.79,
    "duration": 2.55
  },
  {
    "text": "the current incentives are wrong.",
    "start": 551.34,
    "duration": 1.89
  },
  {
    "text": "I mean, we are in an arms race condition,",
    "start": 553.23,
    "duration": 2.13
  },
  {
    "text": "both with respect to\nthe individual companies",
    "start": 555.36,
    "duration": 2.4
  },
  {
    "text": "that are doing this work\nin America and in the West.",
    "start": 557.76,
    "duration": 5.0
  },
  {
    "text": "And we're in an arms race\nwith any other society",
    "start": 562.83,
    "duration": 3.0
  },
  {
    "text": "that is close to getting\ninto the end zone themselves,",
    "start": 565.83,
    "duration": 2.4
  },
  {
    "text": "and I think China being\nthe most obvious example.",
    "start": 568.23,
    "duration": 2.82
  },
  {
    "text": "When you look at what we should\ndo globally, geopolitically,",
    "start": 571.05,
    "duration": 3.42
  },
  {
    "text": "I think we need to win\nthe arms race, right?",
    "start": 574.47,
    "duration": 2.4
  },
  {
    "text": "So that doesn't solve most\nof our problems with AI",
    "start": 576.87,
    "duration": 3.9
  },
  {
    "text": "or much less all of them.",
    "start": 580.77,
    "duration": 1.02
  },
  {
    "text": "But it solves the problem of what would",
    "start": 581.79,
    "duration": 3.664
  },
  {
    "text": "a totalitarian society\nthat is not well disposed",
    "start": 585.454,
    "duration": 5.0
  },
  {
    "text": "to our own do if it had God-like power?",
    "start": 592.89,
    "duration": 5.0
  },
  {
    "text": "And we don't wanna find out, right?",
    "start": 599.04,
    "duration": 2.55
  },
  {
    "text": "So we don't want the\nChinese to get the perfect",
    "start": 601.59,
    "duration": 4.44
  },
  {
    "text": "self-improving AI before we\ndo, or even the imperfect,",
    "start": 606.03,
    "duration": 4.65
  },
  {
    "text": "but nonetheless powerful\nself-improving AI before we do.",
    "start": 610.68,
    "duration": 3.57
  },
  {
    "text": "We don't want them to get\nthe lethal autonomous drones",
    "start": 614.25,
    "duration": 3.78
  },
  {
    "text": "or the robot army before we do.",
    "start": 618.03,
    "duration": 2.313
  },
  {
    "text": "And if my thinking has\nchanged on anything here,",
    "start": 621.42,
    "duration": 3.51
  },
  {
    "text": "and this is going in a\nvery dystopian direction,",
    "start": 624.93,
    "duration": 2.73
  },
  {
    "text": "but, you know, I used to\nthink that we didn't want",
    "start": 627.66,
    "duration": 2.5
  },
  {
    "text": "to militarize and weaponize\nthe most powerful AI",
    "start": 631.14,
    "duration": 4.59
  },
  {
    "text": "as we acquired it.",
    "start": 635.73,
    "duration": 2.066
  },
  {
    "text": "I thought that autonomous weapons",
    "start": 637.796,
    "duration": 2.644
  },
  {
    "text": "was almost definitionally a bad thing.",
    "start": 640.44,
    "duration": 3.48
  },
  {
    "text": "I don't think I believe that anymore.",
    "start": 643.92,
    "duration": 2.07
  },
  {
    "text": "I think it's conceivable\nthat we would build weapons",
    "start": 645.99,
    "duration": 3.12
  },
  {
    "text": "that are autonomous, that\nare better than humans",
    "start": 649.11,
    "duration": 2.61
  },
  {
    "text": "at making judgements with\nrespect to, you know,",
    "start": 651.72,
    "duration": 2.91
  },
  {
    "text": "when to use lethal force, which\nis to say their error rate",
    "start": 654.63,
    "duration": 3.49
  },
  {
    "text": "will be acceptable to us,\neven though they'll have one",
    "start": 659.04,
    "duration": 2.49
  },
  {
    "text": "because it'll simply just be better",
    "start": 661.53,
    "duration": 1.47
  },
  {
    "text": "than keeping a monkey in the loop,",
    "start": 663.0,
    "duration": 2.433
  },
  {
    "text": "in the same way that self-driving cars",
    "start": 666.39,
    "duration": 1.62
  },
  {
    "text": "will almost certainly will\nexist and be better than we are.",
    "start": 668.01,
    "duration": 4.034
  },
  {
    "text": "And at that point, we'll\nconsider it unethical",
    "start": 672.044,
    "duration": 1.786
  },
  {
    "text": "to be driving our own cars\nbecause we'll just, you know,",
    "start": 673.83,
    "duration": 3.0
  },
  {
    "text": "be reliably killing each other\nbecause we're just bad at it.",
    "start": 676.83,
    "duration": 4.8
  },
  {
    "text": "So I think that's conceivable,",
    "start": 681.63,
    "duration": 1.5
  },
  {
    "text": "but more to the point, I\nthink China is guaranteed",
    "start": 683.976,
    "duration": 4.434
  },
  {
    "text": "to attempt to build this kind of weaponry.",
    "start": 688.41,
    "duration": 2.46
  },
  {
    "text": "And what we need are weapons\nthat can counter that.",
    "start": 690.87,
    "duration": 3.54
  },
  {
    "text": "And we need to be more powerful.",
    "start": 694.41,
    "duration": 1.89
  },
  {
    "text": "I just think we're in an arms race",
    "start": 696.3,
    "duration": 1.44
  },
  {
    "text": "that's now unavoidable,\nanalogous to the arms race",
    "start": 697.74,
    "duration": 2.91
  },
  {
    "text": "we had with nuclear weapons.",
    "start": 700.65,
    "duration": 1.23
  },
  {
    "text": "And, unfortunately, the game\ntheory is such that, you know,",
    "start": 701.88,
    "duration": 3.54
  },
  {
    "text": "it's just, we can't opt out.",
    "start": 705.42,
    "duration": 2.1
  },
  {
    "text": "I think it would be bad for us to opt out.",
    "start": 707.52,
    "duration": 2.823
  },
  {
    "text": "That's not a future we wanna live in.",
    "start": 711.693,
    "duration": 1.95
  },
  {
    "text": "That said, I think the\nreal solution ultimately",
    "start": 714.48,
    "duration": 4.14
  },
  {
    "text": "is to achieve a world that\nis politically sane enough",
    "start": 718.62,
    "duration": 4.54
  },
  {
    "text": "that offers enough of\na basis for cooperation",
    "start": 724.14,
    "duration": 2.82
  },
  {
    "text": "at a global scale such that we can get out",
    "start": 726.96,
    "duration": 2.97
  },
  {
    "text": "of this arms race condition.",
    "start": 729.93,
    "duration": 2.58
  },
  {
    "text": "You know, we wanna live in a world where",
    "start": 732.51,
    "duration": 2.22
  },
  {
    "text": "we don't fear the Chinese\nand they don't fear us.",
    "start": 734.73,
    "duration": 5.0
  },
  {
    "text": "And we don't live in a\nworld where at least one",
    "start": 740.52,
    "duration": 2.52
  },
  {
    "text": "of those parties is right to\nbe feeling fear at this moment",
    "start": 743.04,
    "duration": 3.6
  },
  {
    "text": "because the other party\nreally is committed to goals",
    "start": 746.64,
    "duration": 2.52
  },
  {
    "text": "that are inimical to\neverything that we value",
    "start": 749.16,
    "duration": 3.45
  },
  {
    "text": "and are right to value, right?",
    "start": 752.61,
    "duration": 1.5
  },
  {
    "text": "I mean, there are good\nactors and bad actors.",
    "start": 754.11,
    "duration": 2.4
  },
  {
    "text": "It's not that everyone has an equal claim",
    "start": 756.51,
    "duration": 2.58
  },
  {
    "text": "upon the moral high ground here.",
    "start": 759.09,
    "duration": 2.163
  },
  {
    "text": "You know, there's certain human futures",
    "start": 762.9,
    "duration": 2.43
  },
  {
    "text": "that we don't want and we\nare right not to want them.",
    "start": 765.33,
    "duration": 4.38
  },
  {
    "text": "And the fact that you can find millions,",
    "start": 769.71,
    "duration": 2.25
  },
  {
    "text": "and in some cases even billions\nof people who want them,",
    "start": 771.96,
    "duration": 3.51
  },
  {
    "text": "doesn't mean they're not wrong, right?",
    "start": 775.47,
    "duration": 2.43
  },
  {
    "text": "We should get powerful\nAI before those people.",
    "start": 777.9,
    "duration": 3.96
  },
  {
    "text": "I've changed my thinking to some degree",
    "start": 781.86,
    "duration": 1.44
  },
  {
    "text": "on how much we should\nlean into the arms race",
    "start": 783.3,
    "duration": 3.66
  },
  {
    "text": "insofar as it's impossible\nto opt out of it.",
    "start": 786.96,
    "duration": 3.03
  },
  {
    "text": "But ultimately we need\nto move on another track",
    "start": 789.99,
    "duration": 1.65
  },
  {
    "text": "toward something like political sanity",
    "start": 791.64,
    "duration": 2.31
  },
  {
    "text": "where we can step off this current course.",
    "start": 793.95,
    "duration": 4.996
  },
  {
    "text": "- In today's fractured media landscape,",
    "start": 800.07,
    "duration": 2.19
  },
  {
    "text": "cutting through the noise\nto find accurate reporting",
    "start": 802.26,
    "duration": 2.43
  },
  {
    "text": "is harder than ever.",
    "start": 804.69,
    "duration": 1.95
  },
  {
    "text": "Relying on a single source\nlimits your understanding",
    "start": 806.64,
    "duration": 2.58
  },
  {
    "text": "of complex issues,",
    "start": 809.22,
    "duration": 1.56
  },
  {
    "text": "which is why I want to\nshare a powerful tool,",
    "start": 810.78,
    "duration": 2.58
  },
  {
    "text": "Ground News.",
    "start": 813.36,
    "duration": 1.53
  },
  {
    "text": "Ground News aggregates over 50,000 sources",
    "start": 814.89,
    "duration": 2.52
  },
  {
    "text": "from across the political spectrum,",
    "start": 817.41,
    "duration": 2.1
  },
  {
    "text": "letting you compare how different outlets",
    "start": 819.51,
    "duration": 1.74
  },
  {
    "text": "frame the same story.",
    "start": 821.25,
    "duration": 1.8
  },
  {
    "text": "You can see each source's\nbias, factuality,",
    "start": 823.05,
    "duration": 3.0
  },
  {
    "text": "and ownership all in one place,",
    "start": 826.05,
    "duration": 2.55
  },
  {
    "text": "giving you a comprehensive view\nof what's really happening.",
    "start": 828.6,
    "duration": 3.69
  },
  {
    "text": "Take this recent news about\nBaidu's new AI model, ERNIE.",
    "start": 832.29,
    "duration": 4.68
  },
  {
    "text": "One headline frames it\nas a direct challenge",
    "start": 836.97,
    "duration": 2.22
  },
  {
    "text": "to the United States.",
    "start": 839.19,
    "duration": 1.77
  },
  {
    "text": "Meanwhile, another focuses\non the market impact",
    "start": 840.96,
    "duration": 3.0
  },
  {
    "text": "rather than the geopolitical rivalry.",
    "start": 843.96,
    "duration": 2.7
  },
  {
    "text": "You can also use Ground\nNews' Blindspot feature",
    "start": 846.66,
    "duration": 2.7
  },
  {
    "text": "to find stories that are\ndisproportionately reported",
    "start": 849.36,
    "duration": 2.85
  },
  {
    "text": "by either side of the political spectrum.",
    "start": 852.21,
    "duration": 2.58
  },
  {
    "text": "These are stories you might be missing.",
    "start": 854.79,
    "duration": 2.91
  },
  {
    "text": "Right now, you can get\n40% off unlimited access",
    "start": 857.7,
    "duration": 2.88
  },
  {
    "text": "with the Vantage Plan by\nvisiting ground.news/bigthink,",
    "start": 860.58,
    "duration": 4.14
  },
  {
    "text": "or scanning the QR code on screen.",
    "start": 864.72,
    "duration": 2.52
  },
  {
    "text": "This exclusive discount is\navailable through this link.",
    "start": 867.24,
    "duration": 3.09
  },
  {
    "text": "So act now to gain a more\ncomprehensive perspective.",
    "start": 870.33,
    "duration": 4.2
  },
  {
    "text": "Wanna support the channel?",
    "start": 874.53,
    "duration": 1.62
  },
  {
    "text": "Join the Big Think members\ncommunity where you get access",
    "start": 876.15,
    "duration": 2.31
  },
  {
    "text": "to videos early, ad free.",
    "start": 878.46,
    "duration": 2.043
  }
]