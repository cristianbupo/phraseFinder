[
  {
    "text": "In January of 1995, Russia detected\na nuclear missile headed its way.",
    "start": 10.84,
    "duration": 5.964
  },
  {
    "text": "The alert went all the way\nto the president,",
    "start": 16.804,
    "duration": 2.836
  },
  {
    "text": "who was deciding whether to strike back",
    "start": 19.64,
    "duration": 2.962
  },
  {
    "text": "when another system contradicted\nthe initial warning.",
    "start": 22.602,
    "duration": 3.378
  },
  {
    "text": "What they thought was the first missile\nin a massive attack",
    "start": 26.355,
    "duration": 3.796
  },
  {
    "text": "was actually a research rocket\nstudying the Northern Lights.",
    "start": 30.151,
    "duration": 4.338
  },
  {
    "text": "This incident happened \nafter the end of the Cold War,",
    "start": 35.156,
    "duration": 3.42
  },
  {
    "text": "but was nevertheless one of the closest\ncalls we’ve had",
    "start": 38.576,
    "duration": 3.504
  },
  {
    "text": "to igniting a global nuclear war.",
    "start": 42.08,
    "duration": 2.961
  },
  {
    "text": "With the invention of the atomic bomb,",
    "start": 45.833,
    "duration": 2.169
  },
  {
    "text": "humanity gained the power to destroy\nitself for the first time in our history.",
    "start": 48.002,
    "duration": 5.339
  },
  {
    "text": "Since then, our existential risk—",
    "start": 53.341,
    "duration": 3.086
  },
  {
    "text": "risk of either extinction",
    "start": 56.427,
    "duration": 1.835
  },
  {
    "text": "or the unrecoverable collapse \nof human civilization—",
    "start": 58.262,
    "duration": 3.379
  },
  {
    "text": "has steadily increased.",
    "start": 61.641,
    "duration": 1.918
  },
  {
    "text": "It’s well within our power\nto reduce this risk,",
    "start": 64.06,
    "duration": 2.961
  },
  {
    "text": "but in order to do so,",
    "start": 67.021,
    "duration": 1.585
  },
  {
    "text": "we have to understand which \nof our activities",
    "start": 68.606,
    "duration": 2.753
  },
  {
    "text": "pose existential threats now, \nand which might in the future.",
    "start": 71.359,
    "duration": 4.504
  },
  {
    "text": "So far, our species has survived\n2,000 centuries,",
    "start": 76.489,
    "duration": 4.379
  },
  {
    "text": "each with some extinction risk\nfrom natural causes—",
    "start": 80.868,
    "duration": 3.754
  },
  {
    "text": "asteroid impacts, supervolcanoes,\nand the like.",
    "start": 84.622,
    "duration": 3.128
  },
  {
    "text": "Assessing existential risk is \nan inherently uncertain business",
    "start": 88.0,
    "duration": 4.338
  },
  {
    "text": "because usually when we try to figure out\nhow likely something is,",
    "start": 92.338,
    "duration": 3.921
  },
  {
    "text": "we check how often it's happened before.",
    "start": 96.259,
    "duration": 2.46
  },
  {
    "text": "But the complete destruction of humanity\nhas never happened before.",
    "start": 98.719,
    "duration": 4.255
  },
  {
    "text": "While there’s no perfect method to\ndetermine our risk from natural threats,",
    "start": 103.474,
    "duration": 4.004
  },
  {
    "text": "experts estimate it’s about \n1 in 10,000 per century.",
    "start": 107.478,
    "duration": 4.922
  },
  {
    "text": "Nuclear weapons were our first\naddition to that baseline.",
    "start": 112.692,
    "duration": 3.503
  },
  {
    "text": "While there are many risks associated\nwith nuclear weapons,",
    "start": 116.612,
    "duration": 3.17
  },
  {
    "text": "the existential risk comes\nfrom the possibility",
    "start": 119.782,
    "duration": 3.087
  },
  {
    "text": "of a global nuclear war that leads\nto a nuclear winter,",
    "start": 122.869,
    "duration": 4.379
  },
  {
    "text": "where soot from burning cities\nblocks out the sun for years,",
    "start": 127.248,
    "duration": 4.171
  },
  {
    "text": "causing the crops that humanity\ndepends on to fail.",
    "start": 131.419,
    "duration": 3.253
  },
  {
    "text": "We haven't had a nuclear war yet,",
    "start": 135.339,
    "duration": 2.253
  },
  {
    "text": "but our track record is too short to tell\nif they’re inherently unlikely",
    "start": 137.592,
    "duration": 4.421
  },
  {
    "text": "or we’ve simply been lucky.",
    "start": 142.013,
    "duration": 1.71
  },
  {
    "text": "We also can’t say for sure",
    "start": 144.307,
    "duration": 1.459
  },
  {
    "text": "whether a global nuclear war\nwould cause a nuclear winter so severe",
    "start": 145.766,
    "duration": 4.713
  },
  {
    "text": "it would pose an existential threat\nto humanity.",
    "start": 150.479,
    "duration": 2.712
  },
  {
    "text": "The next major addition to our existential\nrisk was climate change.",
    "start": 153.858,
    "duration": 5.047
  },
  {
    "text": "Like nuclear war, \nclimate change could result",
    "start": 159.322,
    "duration": 2.794
  },
  {
    "text": "in a lot of terrible scenarios \nthat we should be working hard to avoid,",
    "start": 162.116,
    "duration": 4.88
  },
  {
    "text": "but that would stop short of causing\nextinction or unrecoverable collapse.",
    "start": 166.996,
    "duration": 4.713
  },
  {
    "text": "We expect a few degrees\nCelsius of warming,",
    "start": 172.043,
    "duration": 2.794
  },
  {
    "text": "but can’t yet completely \nrule out 6 or even 10 degrees,",
    "start": 174.837,
    "duration": 4.171
  },
  {
    "text": "which would cause a calamity of possibly\nunprecedented proportions.",
    "start": 179.008,
    "duration": 4.338
  },
  {
    "text": "Even in this worst-case scenario,",
    "start": 183.596,
    "duration": 2.127
  },
  {
    "text": "it’s not clear whether warming would pose\na direct existential risk,",
    "start": 185.723,
    "duration": 4.046
  },
  {
    "text": "but the disruption it would cause\nwould likely make us more vulnerable",
    "start": 189.769,
    "duration": 3.378
  },
  {
    "text": "to other existential risks.",
    "start": 193.147,
    "duration": 2.002
  },
  {
    "text": "The greatest risks may come \nfrom technologies that are still emerging.",
    "start": 195.566,
    "duration": 4.546
  },
  {
    "text": "Take engineered pandemics.",
    "start": 200.112,
    "duration": 2.294
  },
  {
    "text": "The biggest catastrophes in human history\nhave been from pandemics.",
    "start": 202.406,
    "duration": 4.547
  },
  {
    "text": "And biotechnology is enabling\nus to modify and create germs",
    "start": 207.203,
    "duration": 3.67
  },
  {
    "text": "that could be much more deadly\nthan naturally occurring ones.",
    "start": 210.873,
    "duration": 3.462
  },
  {
    "text": "Such germs could cause pandemics \nthrough biowarfare and research accidents.",
    "start": 214.752,
    "duration": 4.588
  },
  {
    "text": "Decreased costs of genome sequencing\nand modification,",
    "start": 219.34,
    "duration": 3.211
  },
  {
    "text": "along with increased availability\nof potentially dangerous information",
    "start": 222.551,
    "duration": 3.921
  },
  {
    "text": "like the published genomes \nof deadly viruses,",
    "start": 226.472,
    "duration": 2.544
  },
  {
    "text": "also increase the number of people\nand groups",
    "start": 229.016,
    "duration": 2.711
  },
  {
    "text": "who could potentially create\nsuch pathogens.",
    "start": 231.727,
    "duration": 2.92
  },
  {
    "text": "Another concern is unaligned AI.",
    "start": 235.564,
    "duration": 2.878
  },
  {
    "text": "Most AI researchers think this will be\nthe century",
    "start": 238.442,
    "duration": 2.586
  },
  {
    "text": "where we develop artificial intelligence\nthat surpasses human abilities",
    "start": 241.028,
    "duration": 4.338
  },
  {
    "text": "across the board.",
    "start": 245.366,
    "duration": 1.293
  },
  {
    "text": "If we cede this advantage, we place\nour future in the hands",
    "start": 246.659,
    "duration": 4.129
  },
  {
    "text": "of the systems we create.",
    "start": 250.788,
    "duration": 2.002
  },
  {
    "text": "Even if created solely with humanity’s \nbest interests in mind,",
    "start": 252.79,
    "duration": 3.712
  },
  {
    "text": "superintelligent AI could pose \nan existential risk",
    "start": 256.502,
    "duration": 3.504
  },
  {
    "text": "if it isn’t perfectly aligned \nwith human values—",
    "start": 260.006,
    "duration": 3.42
  },
  {
    "text": "a task scientists are finding\nextremely difficult.",
    "start": 263.426,
    "duration": 3.503
  },
  {
    "text": "Based on what we know at this point,",
    "start": 267.346,
    "duration": 2.044
  },
  {
    "text": "some experts estimate the anthropogenic\nexistential risk",
    "start": 269.39,
    "duration": 3.837
  },
  {
    "text": "is more than 100 times higher \nthan the background rate of natural risk.",
    "start": 273.227,
    "duration": 4.797
  },
  {
    "text": "But these odds depend heavily\non human choices.",
    "start": 278.482,
    "duration": 4.004
  },
  {
    "text": "Because most of the risk is from human\naction, and it’s within human control.",
    "start": 282.486,
    "duration": 5.422
  },
  {
    "text": "If we treat safeguarding humanity's future\nas the defining issue of our time,",
    "start": 287.908,
    "duration": 4.755
  },
  {
    "text": "we can reduce this risk.",
    "start": 292.663,
    "duration": 2.127
  },
  {
    "text": "Whether humanity fulfils its potential—",
    "start": 295.082,
    "duration": 2.836
  },
  {
    "text": "or not—",
    "start": 297.918,
    "duration": 1.168
  },
  {
    "text": "is in our hands.",
    "start": 300.129,
    "duration": 1.793
  }
]