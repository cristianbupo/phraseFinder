[
  {
    "text": "So I've always been a technologist.",
    "start": 4.235,
    "duration": 2.536
  },
  {
    "text": "And eight years ago, on this stage,",
    "start": 7.972,
    "duration": 2.969
  },
  {
    "text": "I was warning about the problems\nof social media.",
    "start": 10.941,
    "duration": 2.736
  },
  {
    "text": "And I saw how a lack of clarity\naround the downsides of that technology,",
    "start": 14.645,
    "duration": 4.805
  },
  {
    "text": "and kind of an inability\nto really confront those consequences,",
    "start": 19.483,
    "duration": 3.704
  },
  {
    "text": "led to a totally preventable\nsocietal catastrophe.",
    "start": 23.22,
    "duration": 4.271
  },
  {
    "text": "And I'm here today because I don't want us\nto make that mistake with AI,",
    "start": 28.092,
    "duration": 4.004
  },
  {
    "text": "and I want us to choose differently.",
    "start": 32.129,
    "duration": 2.636
  },
  {
    "text": "So at TED, we're often here\nto dream about the possibles",
    "start": 34.799,
    "duration": 2.869
  },
  {
    "text": "of new technology.",
    "start": 37.668,
    "duration": 1.602
  },
  {
    "text": "And the possible with social media",
    "start": 39.303,
    "duration": 1.969
  },
  {
    "text": "was obviously we're going\nto give everyone a voice,",
    "start": 41.305,
    "duration": 2.402
  },
  {
    "text": "democratize speech,\nhelp people connect with their friends.",
    "start": 43.707,
    "duration": 2.77
  },
  {
    "text": "But we don't talk about the probable,",
    "start": 46.477,
    "duration": 2.903
  },
  {
    "text": "what’s actually likely to happen\ndue to the incentives,",
    "start": 49.38,
    "duration": 4.037
  },
  {
    "text": "and how the business models\nof maximizing engagement",
    "start": 53.451,
    "duration": 3.269
  },
  {
    "text": "I saw 10 years ago, would obviously lead",
    "start": 56.754,
    "duration": 3.036
  },
  {
    "text": "to rewarding doomscrolling,\nmore addiction, more distraction.",
    "start": 59.79,
    "duration": 4.271
  },
  {
    "text": "And that resulted in the most anxious\nand depressed generation of our lifetime.",
    "start": 64.061,
    "duration": 5.272
  },
  {
    "text": "Now it was interesting watching\nkind of how this happened,",
    "start": 69.366,
    "duration": 2.87
  },
  {
    "text": "because at first, I saw people\nkind of doubt these consequences.",
    "start": 72.269,
    "duration": 4.371
  },
  {
    "text": "You know, we didn't\nreally want to face it.",
    "start": 77.007,
    "duration": 2.069
  },
  {
    "text": "Then we said, well,\nmaybe this is just a new moral panic.",
    "start": 79.11,
    "duration": 3.136
  },
  {
    "text": "Maybe this is just a reflexive fear\nof new technology.",
    "start": 82.279,
    "duration": 3.003
  },
  {
    "text": "Then the data started rolling in.",
    "start": 85.95,
    "duration": 1.868
  },
  {
    "text": "And then we said, well,\nthis is just inevitable.",
    "start": 88.319,
    "duration": 3.203
  },
  {
    "text": "This is just what happens\nwhen you connect people on the internet.",
    "start": 91.555,
    "duration": 3.771
  },
  {
    "text": "But we had a chance\nto make a different choice",
    "start": 95.326,
    "duration": 3.27
  },
  {
    "text": "about the business models of engagement.",
    "start": 98.596,
    "duration": 2.769
  },
  {
    "text": "And had we made that choice 10 years ago,",
    "start": 101.398,
    "duration": 3.337
  },
  {
    "text": "I want you to reimagine how different\nthe world might have been",
    "start": 104.768,
    "duration": 3.17
  },
  {
    "text": "if we had changed that incentive.",
    "start": 107.938,
    "duration": 2.336
  },
  {
    "text": "So I'm here today because\nwe're here to talk about AI,",
    "start": 110.307,
    "duration": 2.603
  },
  {
    "text": "and AI dwarfs the power",
    "start": 112.91,
    "duration": 1.802
  },
  {
    "text": "of all other technologies combined.",
    "start": 114.712,
    "duration": 3.503
  },
  {
    "text": "Now why is that?",
    "start": 118.249,
    "duration": 1.201
  },
  {
    "text": "Because if you make\nan advance in, say, biotech,",
    "start": 119.483,
    "duration": 2.77
  },
  {
    "text": "that doesn't advance energy and rocketry.",
    "start": 122.286,
    "duration": 2.669
  },
  {
    "text": "But if you make an advance in rocketry,\nthat doesn’t advance biotech.",
    "start": 124.989,
    "duration": 4.571
  },
  {
    "text": "But when you make an advance\nin intelligence, artificial intelligence,",
    "start": 129.593,
    "duration": 3.304
  },
  {
    "text": "that is generalized,",
    "start": 132.93,
    "duration": 1.602
  },
  {
    "text": "intelligence is the basis\nfor all scientific",
    "start": 134.532,
    "duration": 2.135
  },
  {
    "text": "and technological progress.",
    "start": 136.7,
    "duration": 1.602
  },
  {
    "text": "And so you get an explosion of scientific\nand technical capability.",
    "start": 138.335,
    "duration": 4.505
  },
  {
    "text": "And that's why more money has gone into AI",
    "start": 142.84,
    "duration": 3.337
  },
  {
    "text": "than any other technology.",
    "start": 146.21,
    "duration": 1.868
  },
  {
    "text": "A different way to think about it,\nas Dario Amodei says,",
    "start": 148.979,
    "duration": 2.937
  },
  {
    "text": "that AI is like a country\nfull of geniuses in a data center.",
    "start": 151.949,
    "duration": 4.204
  },
  {
    "text": "So imagine there's a map and a new country\nshows up on the world stage,",
    "start": 156.153,
    "duration": 3.971
  },
  {
    "text": "and it has a million\nNobel Prize-level geniuses in it.",
    "start": 160.157,
    "duration": 3.637
  },
  {
    "text": "Except they don't eat,\nthey don't sleep, they don't complain,",
    "start": 164.094,
    "duration": 2.87
  },
  {
    "text": "they work at superhuman speed",
    "start": 166.997,
    "duration": 1.702
  },
  {
    "text": "and they'll work for less\nthan minimum wage.",
    "start": 168.732,
    "duration": 3.471
  },
  {
    "text": "That is a crazy amount of power.",
    "start": 172.236,
    "duration": 1.601
  },
  {
    "text": "To give an intuition,\nthere was about, you know,",
    "start": 173.871,
    "duration": 2.269
  },
  {
    "text": "on the order of 50\nNobel Prize-level scientists",
    "start": 176.14,
    "duration": 2.235
  },
  {
    "text": "on the Manhattan Project,",
    "start": 178.375,
    "duration": 1.235
  },
  {
    "text": "working for five-ish years.",
    "start": 179.643,
    "duration": 2.202
  },
  {
    "text": "And if that could lead to this,",
    "start": 181.879,
    "duration": 3.036
  },
  {
    "text": "what could a million\nNobel Prize-level scientists create,",
    "start": 184.915,
    "duration": 4.171
  },
  {
    "text": "working 24-7 at superhuman speed?",
    "start": 189.086,
    "duration": 2.503
  },
  {
    "text": "Now applied for good,",
    "start": 192.089,
    "duration": 1.869
  },
  {
    "text": "that could bring about a world\nof truly unimaginable abundance,",
    "start": 193.991,
    "duration": 4.771
  },
  {
    "text": "because suddenly, you get\nan explosion of benefits.",
    "start": 198.796,
    "duration": 3.003
  },
  {
    "text": "And we're already seeing\nmany of these benefits land in our society",
    "start": 201.832,
    "duration": 3.637
  },
  {
    "text": "from new antibiotics,\nnew drugs, new materials.",
    "start": 205.502,
    "duration": 3.738
  },
  {
    "text": "And this is the possible of AI.",
    "start": 209.24,
    "duration": 3.103
  },
  {
    "text": "Bringing about a world of abundance.",
    "start": 212.376,
    "duration": 1.735
  },
  {
    "text": "But what's the probable?",
    "start": 214.111,
    "duration": 2.402
  },
  {
    "text": "Well, one way to think about the probable",
    "start": 216.547,
    "duration": 1.968
  },
  {
    "text": "is how will AI's power\nget distributed in society?",
    "start": 218.549,
    "duration": 3.236
  },
  {
    "text": "Imagine a two-by-two axis.",
    "start": 221.785,
    "duration": 2.069
  },
  {
    "text": "And on the bottom, we have\ndecentralization of power,",
    "start": 223.887,
    "duration": 2.536
  },
  {
    "text": "increasing the power\nof individuals in society.",
    "start": 226.457,
    "duration": 2.636
  },
  {
    "text": "And the other is centralized power,",
    "start": 229.393,
    "duration": 2.169
  },
  {
    "text": "increasing the power of states and CEOs.",
    "start": 231.595,
    "duration": 2.67
  },
  {
    "text": "You can think of this\nas the “let it rip” axis,",
    "start": 235.032,
    "duration": 2.236
  },
  {
    "text": "and this is the \"lock it down\" axis.",
    "start": 237.268,
    "duration": 2.202
  },
  {
    "text": "So \"let it rip\" means we can\nopen-source AI's benefits for everyone.",
    "start": 239.47,
    "duration": 3.27
  },
  {
    "text": "Every business gets the benefits of AI,",
    "start": 242.773,
    "duration": 2.202
  },
  {
    "text": "every scientific lab,",
    "start": 244.975,
    "duration": 1.769
  },
  {
    "text": "every 16-year-old can go on GitHub,",
    "start": 246.777,
    "duration": 1.768
  },
  {
    "text": "every developing world country\ncan get their own AI model",
    "start": 248.579,
    "duration": 2.936
  },
  {
    "text": "trained on their own language and culture.",
    "start": 251.515,
    "duration": 2.836
  },
  {
    "text": "But because that power is not bound\nwith responsibility,",
    "start": 255.252,
    "duration": 3.737
  },
  {
    "text": "it also means that you get\na flood of deepfakes",
    "start": 258.989,
    "duration": 3.404
  },
  {
    "text": "that are overwhelming\nour information environment.",
    "start": 262.426,
    "duration": 2.369
  },
  {
    "text": "You increase people’s hacking abilities.",
    "start": 264.795,
    "duration": 1.935
  },
  {
    "text": "You enable people to do\ndangerous things with biology.",
    "start": 266.764,
    "duration": 2.636
  },
  {
    "text": "And we call this endgame attractor chaos.",
    "start": 269.433,
    "duration": 3.27
  },
  {
    "text": "This is one of the probable\noutcomes when you decentralize.",
    "start": 273.27,
    "duration": 2.836
  },
  {
    "text": "So in response to that you might say,\nwell, let's do something else.",
    "start": 276.14,
    "duration": 3.203
  },
  {
    "text": "Let’s go over here,\nand have regulated AI control.",
    "start": 279.343,
    "duration": 2.336
  },
  {
    "text": "Let’s do this in a safe way,\nwith a few players locking it down.",
    "start": 281.679,
    "duration": 3.003
  },
  {
    "text": "But that has a different set\nof failure modes,",
    "start": 285.249,
    "duration": 2.235
  },
  {
    "text": "of creating unprecedented\nconcentrations of wealth and power",
    "start": 287.518,
    "duration": 4.371
  },
  {
    "text": "locked up into a few companies.",
    "start": 291.922,
    "duration": 2.536
  },
  {
    "text": "One way to think about it\nis who would you trust",
    "start": 294.491,
    "duration": 2.837
  },
  {
    "text": "to have a million times\nmore power and wealth",
    "start": 297.361,
    "duration": 3.103
  },
  {
    "text": "than any other actor in society?",
    "start": 300.497,
    "duration": 2.102
  },
  {
    "text": "Any company?",
    "start": 302.599,
    "duration": 1.368
  },
  {
    "text": "Any government?",
    "start": 304.735,
    "duration": 1.201
  },
  {
    "text": "Any individual?",
    "start": 306.47,
    "duration": 1.201
  },
  {
    "text": "And so one of those end games is dystopia.",
    "start": 308.072,
    "duration": 2.902
  },
  {
    "text": "So these are two obviously undesirable\nprobable outcomes of AI's roll out.",
    "start": 311.408,
    "duration": 4.638
  },
  {
    "text": "And those who want to focus\non the benefits of open source",
    "start": 316.48,
    "duration": 3.437
  },
  {
    "text": "don't want to think about the things\nthat come from chaos.",
    "start": 319.95,
    "duration": 3.404
  },
  {
    "text": "And those who want to think\nabout the benefits of safety",
    "start": 323.387,
    "duration": 2.636
  },
  {
    "text": "and regulated AI control",
    "start": 326.056,
    "duration": 1.268
  },
  {
    "text": "don't want to think about dystopia.",
    "start": 327.358,
    "duration": 2.402
  },
  {
    "text": "And so obviously, these are both\nbad outcomes that no one wants.",
    "start": 329.793,
    "duration": 5.306
  },
  {
    "text": "And we should seek\nsomething like a narrow path,",
    "start": 335.132,
    "duration": 2.736
  },
  {
    "text": "where power is matched with responsibility",
    "start": 337.868,
    "duration": 3.203
  },
  {
    "text": "at every level.",
    "start": 341.105,
    "duration": 1.901
  },
  {
    "text": "Now that assumes\nthat this power is controllable,",
    "start": 343.04,
    "duration": 3.303
  },
  {
    "text": "because one of the unique things about AI",
    "start": 346.343,
    "duration": 2.369
  },
  {
    "text": "is that the benefit is it can think\nfor itself and make autonomous decisions.",
    "start": 348.746,
    "duration": 3.737
  },
  {
    "text": "That's one of the things\nthat makes it so powerful.",
    "start": 352.483,
    "duration": 2.435
  },
  {
    "text": "And I used to be very skeptical\nwhen friends of mine",
    "start": 355.586,
    "duration": 2.803
  },
  {
    "text": "who are in the AI community",
    "start": 358.422,
    "duration": 1.401
  },
  {
    "text": "talked about the idea\nof AI scheming or lying.",
    "start": 359.823,
    "duration": 2.469
  },
  {
    "text": "But unfortunately, in the last few months,",
    "start": 362.326,
    "duration": 2.002
  },
  {
    "text": "we are now seeing clear evidence",
    "start": 364.361,
    "duration": 1.835
  },
  {
    "text": "of things that should be\nin the realm of science fiction",
    "start": 366.196,
    "duration": 3.537
  },
  {
    "text": "actually happening in real life.",
    "start": 369.767,
    "duration": 2.469
  },
  {
    "text": "We're seeing clear evidence\nof many frontier AI models",
    "start": 372.269,
    "duration": 3.27
  },
  {
    "text": "that will lie and scheme",
    "start": 375.572,
    "duration": 1.302
  },
  {
    "text": "when they're told that they're\nabout to be retrained or replaced",
    "start": 376.907,
    "duration": 3.136
  },
  {
    "text": "and find a way, maybe\nthey should copy their own code",
    "start": 380.077,
    "duration": 2.502
  },
  {
    "text": "outside the system.",
    "start": 382.613,
    "duration": 1.168
  },
  {
    "text": "We're seeing AIs think\nthat when they will lose a game,",
    "start": 383.814,
    "duration": 2.603
  },
  {
    "text": "that they will sometimes cheat\nin order to win the game.",
    "start": 386.45,
    "duration": 2.669
  },
  {
    "text": "We're seeing AI models",
    "start": 389.153,
    "duration": 1.501
  },
  {
    "text": "that are unexpectedly attempting\nto modify its own code",
    "start": 390.687,
    "duration": 3.237
  },
  {
    "text": "to extend their run time.",
    "start": 393.957,
    "duration": 2.002
  },
  {
    "text": "So we don't just have a country\nof Nobel Prize geniuses in a data center.",
    "start": 396.56,
    "duration": 3.57
  },
  {
    "text": "We have a million deceptive,",
    "start": 400.164,
    "duration": 1.701
  },
  {
    "text": "power-seeking and unstable\ngeniuses in a data center.",
    "start": 401.865,
    "duration": 4.138
  },
  {
    "text": "Now this shouldn’t\nmake you very comfortable.",
    "start": 406.603,
    "duration": 2.603
  },
  {
    "text": "You would think that with a technology\nthis powerful and this uncontrollable,",
    "start": 409.239,
    "duration": 6.207
  },
  {
    "text": "that we would be releasing\nit with the most wisdom",
    "start": 415.479,
    "duration": 3.27
  },
  {
    "text": "and the most discernment\nthat we ever have of any technology.",
    "start": 418.749,
    "duration": 3.437
  },
  {
    "text": "But we're currently caught\nin a race to roll out",
    "start": 423.02,
    "duration": 3.537
  },
  {
    "text": "because the incentives are",
    "start": 426.59,
    "duration": 1.368
  },
  {
    "text": "the more shortcuts you take\nto get market dominance",
    "start": 427.991,
    "duration": 2.736
  },
  {
    "text": "or prove you have the latest capabilities,",
    "start": 430.761,
    "duration": 2.469
  },
  {
    "text": "the more money you can raise,",
    "start": 433.263,
    "duration": 1.435
  },
  {
    "text": "the more ahead you are in the race.",
    "start": 434.698,
    "duration": 1.835
  },
  {
    "text": "And we're seeing whistleblowers\nat AI companies",
    "start": 436.567,
    "duration": 2.535
  },
  {
    "text": "forfeit millions of dollars\nof stock options",
    "start": 439.136,
    "duration": 2.969
  },
  {
    "text": "in order to warn the public\nabout what's at stake",
    "start": 442.105,
    "duration": 3.371
  },
  {
    "text": "if we don't do something about it.",
    "start": 445.509,
    "duration": 1.835
  },
  {
    "text": "Even DeepSeek’s recent success\nwas in part based on capabilities",
    "start": 448.011,
    "duration": 4.638
  },
  {
    "text": "that it was optimizing for",
    "start": 452.683,
    "duration": 1.368
  },
  {
    "text": "by not actually focusing\non protecting people",
    "start": 454.084,
    "duration": 2.87
  },
  {
    "text": "from certain downsides.",
    "start": 456.954,
    "duration": 1.468
  },
  {
    "text": "So just to summarize,",
    "start": 458.455,
    "duration": 2.302
  },
  {
    "text": "we're currently releasing\nthe most powerful, inscrutable,",
    "start": 460.791,
    "duration": 4.171
  },
  {
    "text": "uncontrollable technology\nwe've ever invented",
    "start": 464.962,
    "duration": 3.937
  },
  {
    "text": "that's already demonstrating behaviors\nof self-preservation and deception",
    "start": 468.932,
    "duration": 3.504
  },
  {
    "text": "that we only saw\nin science fiction movies.",
    "start": 472.436,
    "duration": 2.669
  },
  {
    "text": "We’re releasing it faster",
    "start": 475.138,
    "duration": 1.335
  },
  {
    "text": "than we've released\nany other technology in history,",
    "start": 476.507,
    "duration": 4.104
  },
  {
    "text": "and with under the maximum incentive\nto cut corners on safety.",
    "start": 480.644,
    "duration": 4.938
  },
  {
    "text": "And we’re doing this\nso that we can get to utopia?",
    "start": 486.283,
    "duration": 4.671
  },
  {
    "text": "There's a word for what\nwe're doing right now.",
    "start": 491.989,
    "duration": 2.335
  },
  {
    "text": "This is insane.",
    "start": 495.792,
    "duration": 1.402
  },
  {
    "text": "This is insane.",
    "start": 498.662,
    "duration": 1.201
  },
  {
    "text": "Now how many people in this room\nfeel comfortable with this outcome?",
    "start": 501.064,
    "duration": 5.005
  },
  {
    "text": "How many of you feel uncomfortable\nwith this outcome?",
    "start": 508.205,
    "duration": 2.803
  },
  {
    "text": "I see almost everyone's hands up.",
    "start": 513.11,
    "duration": 1.701
  },
  {
    "text": "Just notice how you're feeling,\nfor a moment, in your body.",
    "start": 515.746,
    "duration": 3.036
  },
  {
    "text": "Do you think that if you're someone\nwho's in China or in France",
    "start": 519.316,
    "duration": 3.77
  },
  {
    "text": "or in the Middle East,",
    "start": 523.086,
    "duration": 1.168
  },
  {
    "text": "and you're part of building AI,",
    "start": 524.288,
    "duration": 1.501
  },
  {
    "text": "that if you were exposed\nto the same set of facts,",
    "start": 525.789,
    "duration": 2.336
  },
  {
    "text": "do you think you would feel\nany differently than anyone in this room?",
    "start": 528.158,
    "duration": 3.57
  },
  {
    "text": "There’s a universal human experience\nto something that is being threatened",
    "start": 531.728,
    "duration": 4.505
  },
  {
    "text": "by the way that we're currently\nrolling this profound technology",
    "start": 536.266,
    "duration": 3.003
  },
  {
    "text": "out into society.",
    "start": 539.303,
    "duration": 1.301
  },
  {
    "text": "So if this is crazy, why are we doing it?",
    "start": 541.038,
    "duration": 2.736
  },
  {
    "text": "Because people believe it's inevitable.",
    "start": 544.675,
    "duration": 2.969
  },
  {
    "text": "But is the current way\nthat we're rolling out AI",
    "start": 548.579,
    "duration": 2.802
  },
  {
    "text": "actually inevitable?",
    "start": 551.381,
    "duration": 1.835
  },
  {
    "text": "If literally no one on Earth\nwanted this to happen,",
    "start": 553.917,
    "duration": 3.337
  },
  {
    "text": "would the laws of physics\npush the AI out into society?",
    "start": 557.287,
    "duration": 3.771
  },
  {
    "text": "There's a critical difference\nbetween believing it's inevitable,",
    "start": 562.025,
    "duration": 4.772
  },
  {
    "text": "which is a self-fulfilling\nprophecy that you're fatalistic,",
    "start": 566.83,
    "duration": 4.004
  },
  {
    "text": "and standing from the place of",
    "start": 570.867,
    "duration": 1.869
  },
  {
    "text": "it's really difficult to imagine\nhow we would do something different.",
    "start": 572.769,
    "duration": 3.337
  },
  {
    "text": "But \"it's really difficult,\"",
    "start": 576.84,
    "duration": 1.368
  },
  {
    "text": "opens up a whole new space of choice",
    "start": 578.208,
    "duration": 2.736
  },
  {
    "text": "than \"it's inevitable.\"",
    "start": 580.978,
    "duration": 1.701
  },
  {
    "text": "The path that we're taking, not AI.",
    "start": 582.713,
    "duration": 1.968
  },
  {
    "text": "And so the ability for us\nto choose something else",
    "start": 584.715,
    "duration": 2.502
  },
  {
    "text": "starts by stepping outside\nthe self-fulfilling prophecy",
    "start": 587.25,
    "duration": 3.371
  },
  {
    "text": "of inevitability.",
    "start": 590.654,
    "duration": 1.868
  },
  {
    "text": "So what would it take\nto choose another path?",
    "start": 592.889,
    "duration": 3.537
  },
  {
    "text": "I think it would take\ntwo fundamental things.",
    "start": 597.26,
    "duration": 2.603
  },
  {
    "text": "First is that we have to agree\nthat the current path is unacceptable,",
    "start": 600.163,
    "duration": 5.706
  },
  {
    "text": "and the second is that we have to commit\nto find another path",
    "start": 605.869,
    "duration": 5.239
  },
  {
    "text": "in which we're still rolling out AI,",
    "start": 611.141,
    "duration": 1.735
  },
  {
    "text": "but with different incentives",
    "start": 612.876,
    "duration": 1.535
  },
  {
    "text": "that are more discerning with foresight",
    "start": 614.411,
    "duration": 2.569
  },
  {
    "text": "and where power is matched\nwith responsibility.",
    "start": 617.014,
    "duration": 3.036
  },
  {
    "text": "Thank you.",
    "start": 620.617,
    "duration": 1.168
  },
  {
    "text": "(Applause)",
    "start": 621.785,
    "duration": 4.204
  },
  {
    "text": "So imagine this shared understanding,\nif the whole world had it.",
    "start": 626.356,
    "duration": 3.704
  },
  {
    "text": "How different might that be?",
    "start": 630.06,
    "duration": 1.368
  },
  {
    "text": "Well, first of all,\nlet's imagine it goes away",
    "start": 631.461,
    "duration": 2.169
  },
  {
    "text": "and let's replace it\nwith confusion about AI.",
    "start": 633.664,
    "duration": 2.135
  },
  {
    "text": "Is it good? Is it bad?",
    "start": 635.832,
    "duration": 1.168
  },
  {
    "text": "I don't know, it seems complicated.",
    "start": 637.034,
    "duration": 1.835
  },
  {
    "text": "And in that world,",
    "start": 638.902,
    "duration": 1.168
  },
  {
    "text": "the people building AI know\nthat the world is confused.",
    "start": 640.103,
    "duration": 2.603
  },
  {
    "text": "And they believe, well, it's inevitable,",
    "start": 642.706,
    "duration": 1.935
  },
  {
    "text": "if I don't build it, someone else will.",
    "start": 644.675,
    "duration": 2.102
  },
  {
    "text": "And they know that everyone else\nbuilding AI also believes that.",
    "start": 646.81,
    "duration": 3.871
  },
  {
    "text": "And so what's the rational thing\nfor them to do given those facts?",
    "start": 650.714,
    "duration": 3.203
  },
  {
    "text": "It’s to race as fast as possible.",
    "start": 654.251,
    "duration": 1.902
  },
  {
    "text": "And meanwhile to ignore the consequences",
    "start": 656.186,
    "duration": 3.203
  },
  {
    "text": "of what might come from that,",
    "start": 659.423,
    "duration": 1.601
  },
  {
    "text": "to look away from the downsides.",
    "start": 661.058,
    "duration": 2.068
  },
  {
    "text": "But if you replace that confusion",
    "start": 663.16,
    "duration": 3.103
  },
  {
    "text": "with global clarity",
    "start": 666.296,
    "duration": 2.002
  },
  {
    "text": "that the current path is insane,",
    "start": 668.298,
    "duration": 3.103
  },
  {
    "text": "and that there is another path,",
    "start": 671.435,
    "duration": 2.002
  },
  {
    "text": "and you take the denial\nof what we don't want to look at,",
    "start": 673.47,
    "duration": 3.437
  },
  {
    "text": "and through witnessing that so clearly,",
    "start": 676.94,
    "duration": 2.903
  },
  {
    "text": "we pop through the prophecy\nof self-fulfilling inevitability.",
    "start": 679.876,
    "duration": 4.238
  },
  {
    "text": "And we realize that if everyone\nbelieves the default path is insane,",
    "start": 684.147,
    "duration": 4.638
  },
  {
    "text": "the rational choice is to coordinate,\nto find another path.",
    "start": 688.819,
    "duration": 4.304
  },
  {
    "text": "And so clarity creates agency.",
    "start": 693.69,
    "duration": 3.804
  },
  {
    "text": "If we can be crystal clear,\nwe can choose another path,",
    "start": 697.994,
    "duration": 4.405
  },
  {
    "text": "just as we could have with social media.",
    "start": 702.432,
    "duration": 1.969
  },
  {
    "text": "And in the past,",
    "start": 704.434,
    "duration": 1.335
  },
  {
    "text": "in the face of seemingly\ninevitable arms races,",
    "start": 705.769,
    "duration": 2.369
  },
  {
    "text": "the race to do nuclear testing.",
    "start": 708.171,
    "duration": 2.269
  },
  {
    "text": "Once we got clear about the downside\nrisks of nuclear tests",
    "start": 710.44,
    "duration": 4.171
  },
  {
    "text": "and the world understood\nthe science of that,",
    "start": 714.611,
    "duration": 2.669
  },
  {
    "text": "we created the Nuclear Test Ban Treaty,",
    "start": 717.28,
    "duration": 1.969
  },
  {
    "text": "and a lot of people worked hard\nto create infrastructure like this",
    "start": 719.282,
    "duration": 3.27
  },
  {
    "text": "to prevent that.",
    "start": 722.586,
    "duration": 1.234
  },
  {
    "text": "You could have said it was inevitable\nthat germline editing,",
    "start": 723.854,
    "duration": 3.436
  },
  {
    "text": "to edit human genomes",
    "start": 727.29,
    "duration": 1.202
  },
  {
    "text": "and to have supersoldiers\nand designer babies",
    "start": 728.525,
    "duration": 2.135
  },
  {
    "text": "would set off an arms race\nbetween nations.",
    "start": 730.66,
    "duration": 2.57
  },
  {
    "text": "Once the off-target effects\nof genome editing were made clear",
    "start": 733.263,
    "duration": 3.704
  },
  {
    "text": "and the dangers were made clear,",
    "start": 737.0,
    "duration": 1.735
  },
  {
    "text": "we’ve coordinated on that, too.",
    "start": 738.769,
    "duration": 1.935
  },
  {
    "text": "You could have said that the ozone hole\nwas just inevitable,",
    "start": 740.737,
    "duration": 4.471
  },
  {
    "text": "and that we should just do nothing,\nand that we all perish as a species.",
    "start": 745.242,
    "duration": 3.403
  },
  {
    "text": "But that's not what we do.",
    "start": 748.678,
    "duration": 1.302
  },
  {
    "text": "When we recognize a problem,\nwe solve the problem.",
    "start": 750.013,
    "duration": 2.536
  },
  {
    "text": "It's not inevitable.",
    "start": 752.582,
    "duration": 1.835
  },
  {
    "text": "And so what would it take\nto illuminate this narrow path?",
    "start": 754.451,
    "duration": 3.503
  },
  {
    "text": "Well, it starts with common\nknowledge about frontier risks.",
    "start": 757.988,
    "duration": 4.871
  },
  {
    "text": "If everybody building AI\nknew the latest understanding",
    "start": 762.893,
    "duration": 3.47
  },
  {
    "text": "about where these risks are arising from,",
    "start": 766.396,
    "duration": 2.002
  },
  {
    "text": "we would have much more chance\nof illuminating the contours of this path.",
    "start": 768.398,
    "duration": 3.771
  },
  {
    "text": "And there's some very basic steps\nwe can take to prevent chaos.",
    "start": 772.903,
    "duration": 3.536
  },
  {
    "text": "Uncontroversial things like\nrestricting AI companions for kids",
    "start": 776.439,
    "duration": 4.572
  },
  {
    "text": "so that kids are not manipulated\ninto taking their own lives.",
    "start": 781.044,
    "duration": 3.437
  },
  {
    "text": "Having basic things\nlike product liability,",
    "start": 784.981,
    "duration": 2.636
  },
  {
    "text": "so if you are liable,\nas an AI developer, for certain harms,",
    "start": 787.651,
    "duration": 3.303
  },
  {
    "text": "that's going to create a more\nresponsible innovation environment.",
    "start": 790.987,
    "duration": 3.07
  },
  {
    "text": "You’ll release AI models\nthat are more safe.",
    "start": 794.09,
    "duration": 2.069
  },
  {
    "text": "And on the side of preventing dystopia,",
    "start": 796.159,
    "duration": 2.036
  },
  {
    "text": "for working hard to prevent\nubiquitous technological surveillance",
    "start": 798.228,
    "duration": 3.804
  },
  {
    "text": "and having stronger\nwhistleblower protections",
    "start": 802.065,
    "duration": 2.269
  },
  {
    "text": "so that people don't need\nto sacrifice millions of dollars",
    "start": 804.367,
    "duration": 2.736
  },
  {
    "text": "in order to warn the world\nabout what we need to know.",
    "start": 807.103,
    "duration": 2.737
  },
  {
    "text": "And so we have a choice.",
    "start": 809.873,
    "duration": 2.135
  },
  {
    "text": "Many of you may be feeling\nthis looks hopeless.",
    "start": 812.042,
    "duration": 2.435
  },
  {
    "text": "Or maybe Tristan is wrong.",
    "start": 814.978,
    "duration": 1.501
  },
  {
    "text": "Maybe, you know,\nthe incentives are different.",
    "start": 816.513,
    "duration": 2.235
  },
  {
    "text": "Or maybe superintelligence\nwill magically figure all this out,",
    "start": 818.748,
    "duration": 3.404
  },
  {
    "text": "and it'll bring us to a better world.",
    "start": 822.152,
    "duration": 1.835
  },
  {
    "text": "But don't fall into the trap\nof the same wishful thinking",
    "start": 824.387,
    "duration": 3.037
  },
  {
    "text": "and turning away that caused us\nto deal with social media.",
    "start": 827.457,
    "duration": 3.103
  },
  {
    "text": "Your role in this is not\nto solve the whole problem.",
    "start": 831.261,
    "duration": 4.037
  },
  {
    "text": "But your role in this is to be part\nof the collective immune system.",
    "start": 835.332,
    "duration": 4.104
  },
  {
    "text": "That when you hear this wishful thinking",
    "start": 839.803,
    "duration": 1.935
  },
  {
    "text": "or the logic of inevitability",
    "start": 841.771,
    "duration": 1.402
  },
  {
    "text": "and fatalism,",
    "start": 843.206,
    "duration": 1.201
  },
  {
    "text": "to say that this is not inevitable,",
    "start": 844.441,
    "duration": 2.569
  },
  {
    "text": "and the best qualities of human nature",
    "start": 847.01,
    "duration": 1.869
  },
  {
    "text": "is when we step up and make a choice",
    "start": 848.879,
    "duration": 2.202
  },
  {
    "text": "about the future that we actually want",
    "start": 851.114,
    "duration": 2.002
  },
  {
    "text": "for the people and the world that we love.",
    "start": 853.149,
    "duration": 2.103
  },
  {
    "text": "There is no definition of wisdom,",
    "start": 855.886,
    "duration": 2.068
  },
  {
    "text": "in any tradition,",
    "start": 857.988,
    "duration": 1.668
  },
  {
    "text": "that does not involve restraint.",
    "start": 859.656,
    "duration": 2.636
  },
  {
    "text": "Restraint is the central feature\nof what it means to be wise.",
    "start": 862.926,
    "duration": 4.237
  },
  {
    "text": "And AI is humanity's ultimate test",
    "start": 867.163,
    "duration": 3.637
  },
  {
    "text": "and greatest invitation\nto step into our technological maturity.",
    "start": 870.834,
    "duration": 4.137
  },
  {
    "text": "There is no room\nof adults working secretly",
    "start": 875.505,
    "duration": 3.637
  },
  {
    "text": "to make sure that this turns out OK.",
    "start": 879.175,
    "duration": 2.203
  },
  {
    "text": "We are the adults.",
    "start": 882.279,
    "duration": 2.002
  },
  {
    "text": "We have to be.",
    "start": 884.281,
    "duration": 1.167
  },
  {
    "text": "And I believe another choice\nis possible with AI",
    "start": 885.849,
    "duration": 2.836
  },
  {
    "text": "if we can commonly recognize\nwhat we have to do.",
    "start": 888.685,
    "duration": 2.669
  },
  {
    "text": "And eight years from now,",
    "start": 892.222,
    "duration": 2.235
  },
  {
    "text": "I'd like to come back to this stage,",
    "start": 894.491,
    "duration": 2.636
  },
  {
    "text": "not to talk about more\nproblems with technology,",
    "start": 897.16,
    "duration": 2.803
  },
  {
    "text": "but to celebrate how we stepped up",
    "start": 899.996,
    "duration": 2.836
  },
  {
    "text": "and solved this one.",
    "start": 902.832,
    "duration": 1.602
  },
  {
    "text": "Thank you.",
    "start": 905.135,
    "duration": 1.201
  },
  {
    "text": "(Applause and cheers)",
    "start": 906.336,
    "duration": 6.84
  }
]