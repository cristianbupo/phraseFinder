[
  {
    "text": "Transcriber: Leslie Gauthier\nReviewer: Joanna Pietrulewicz",
    "start": 0.0,
    "duration": 7.0
  },
  {
    "text": "Every day, every week,",
    "start": 12.792,
    "duration": 2.267
  },
  {
    "text": "we agree to terms and conditions.",
    "start": 15.083,
    "duration": 2.185
  },
  {
    "text": "And when we do this,",
    "start": 17.292,
    "duration": 1.476
  },
  {
    "text": "we provide companies with the lawful right",
    "start": 18.792,
    "duration": 2.476
  },
  {
    "text": "to do whatever they want with our data",
    "start": 21.292,
    "duration": 3.684
  },
  {
    "text": "and with the data of our children.",
    "start": 25.0,
    "duration": 2.375
  },
  {
    "text": "Which makes us wonder:",
    "start": 28.792,
    "duration": 2.976
  },
  {
    "text": "how much data are we giving\naway of children,",
    "start": 31.792,
    "duration": 2.892
  },
  {
    "text": "and what are its implications?",
    "start": 34.708,
    "duration": 2.0
  },
  {
    "text": "I'm an anthropologist,",
    "start": 38.5,
    "duration": 1.393
  },
  {
    "text": "and I'm also the mother\nof two little girls.",
    "start": 39.917,
    "duration": 2.601
  },
  {
    "text": "And I started to become interested\nin this question in 2015",
    "start": 42.542,
    "duration": 4.476
  },
  {
    "text": "when I suddenly realized\nthat there were vast --",
    "start": 47.042,
    "duration": 2.726
  },
  {
    "text": "almost unimaginable amounts of data traces",
    "start": 49.792,
    "duration": 3.017
  },
  {
    "text": "that are being produced\nand collected about children.",
    "start": 52.833,
    "duration": 3.167
  },
  {
    "text": "So I launched a research project,",
    "start": 56.792,
    "duration": 1.976
  },
  {
    "text": "which is called Child Data Citizen,",
    "start": 58.792,
    "duration": 2.476
  },
  {
    "text": "and I aimed at filling in the blank.",
    "start": 61.292,
    "duration": 2.125
  },
  {
    "text": "Now you may think\nthat I'm here to blame you",
    "start": 64.583,
    "duration": 3.018
  },
  {
    "text": "for posting photos\nof your children on social media,",
    "start": 67.625,
    "duration": 2.768
  },
  {
    "text": "but that's not really the point.",
    "start": 70.417,
    "duration": 2.142
  },
  {
    "text": "The problem is way bigger\nthan so-called \"sharenting.\"",
    "start": 72.583,
    "duration": 3.417
  },
  {
    "text": "This is about systems, not individuals.",
    "start": 76.792,
    "duration": 4.101
  },
  {
    "text": "You and your habits are not to blame.",
    "start": 80.917,
    "duration": 2.291
  },
  {
    "text": "For the very first time in history,",
    "start": 84.833,
    "duration": 2.851
  },
  {
    "text": "we are tracking\nthe individual data of children",
    "start": 87.708,
    "duration": 2.56
  },
  {
    "text": "from long before they're born --",
    "start": 90.292,
    "duration": 1.767
  },
  {
    "text": "sometimes from the moment of conception,",
    "start": 92.083,
    "duration": 2.685
  },
  {
    "text": "and then throughout their lives.",
    "start": 94.792,
    "duration": 2.351
  },
  {
    "text": "You see, when parents decide to conceive,",
    "start": 97.167,
    "duration": 3.101
  },
  {
    "text": "they go online to look\nfor \"ways to get pregnant,\"",
    "start": 100.292,
    "duration": 2.976
  },
  {
    "text": "or they download ovulation-tracking apps.",
    "start": 103.292,
    "duration": 2.75
  },
  {
    "text": "When they do get pregnant,",
    "start": 107.25,
    "duration": 2.601
  },
  {
    "text": "they post ultrasounds\nof their babies on social media,",
    "start": 109.875,
    "duration": 3.143
  },
  {
    "text": "they download pregnancy apps",
    "start": 113.042,
    "duration": 2.017
  },
  {
    "text": "or they consult Dr. Google\nfor all sorts of things,",
    "start": 115.083,
    "duration": 3.726
  },
  {
    "text": "like, you know --",
    "start": 118.833,
    "duration": 1.518
  },
  {
    "text": "for \"miscarriage risk when flying\"",
    "start": 120.375,
    "duration": 2.559
  },
  {
    "text": "or \"abdominal cramps in early pregnancy.\"",
    "start": 122.958,
    "duration": 2.768
  },
  {
    "text": "I know because I've done it --",
    "start": 125.75,
    "duration": 1.809
  },
  {
    "text": "and many times.",
    "start": 127.583,
    "duration": 1.625
  },
  {
    "text": "And then, when the baby is born,\nthey track every nap,",
    "start": 130.458,
    "duration": 2.81
  },
  {
    "text": "every feed,",
    "start": 133.292,
    "duration": 1.267
  },
  {
    "text": "every life event\non different technologies.",
    "start": 134.583,
    "duration": 2.584
  },
  {
    "text": "And all of these technologies",
    "start": 138.083,
    "duration": 1.476
  },
  {
    "text": "transform the baby's most intimate\nbehavioral and health data into profit",
    "start": 139.583,
    "duration": 6.143
  },
  {
    "text": "by sharing it with others.",
    "start": 145.75,
    "duration": 1.792
  },
  {
    "text": "So to give you an idea of how this works,",
    "start": 148.583,
    "duration": 2.143
  },
  {
    "text": "in 2019, the British Medical Journal\npublished research that showed",
    "start": 150.75,
    "duration": 5.184
  },
  {
    "text": "that out of 24 mobile health apps,",
    "start": 155.958,
    "duration": 3.643
  },
  {
    "text": "19 shared information with third parties.",
    "start": 159.625,
    "duration": 3.458
  },
  {
    "text": "And these third parties shared information\nwith 216 other organizations.",
    "start": 164.083,
    "duration": 5.834
  },
  {
    "text": "Of these 216 other fourth parties,",
    "start": 170.875,
    "duration": 3.434
  },
  {
    "text": "only three belonged to the health sector.",
    "start": 174.333,
    "duration": 3.143
  },
  {
    "text": "The other companies that had access\nto that data were big tech companies",
    "start": 177.5,
    "duration": 4.518
  },
  {
    "text": "like Google, Facebook or Oracle,",
    "start": 182.042,
    "duration": 3.517
  },
  {
    "text": "they were digital advertising companies",
    "start": 185.583,
    "duration": 2.601
  },
  {
    "text": "and there was also\na consumer credit reporting agency.",
    "start": 188.208,
    "duration": 4.125
  },
  {
    "text": "So you get it right:",
    "start": 193.125,
    "duration": 1.434
  },
  {
    "text": "ad companies and credit agencies may\nalready have data points on little babies.",
    "start": 194.583,
    "duration": 5.125
  },
  {
    "text": "But mobile apps,\nweb searches and social media",
    "start": 201.125,
    "duration": 2.768
  },
  {
    "text": "are really just the tip of the iceberg,",
    "start": 203.917,
    "duration": 3.101
  },
  {
    "text": "because children are being tracked\nby multiple technologies",
    "start": 207.042,
    "duration": 2.851
  },
  {
    "text": "in their everyday lives.",
    "start": 209.917,
    "duration": 1.726
  },
  {
    "text": "They're tracked by home technologies\nand virtual assistants in their homes.",
    "start": 211.667,
    "duration": 4.142
  },
  {
    "text": "They're tracked by educational platforms",
    "start": 215.833,
    "duration": 1.976
  },
  {
    "text": "and educational technologies\nin their schools.",
    "start": 217.833,
    "duration": 2.185
  },
  {
    "text": "They're tracked by online records",
    "start": 220.042,
    "duration": 1.601
  },
  {
    "text": "and online portals\nat their doctor's office.",
    "start": 221.667,
    "duration": 3.017
  },
  {
    "text": "They're tracked by their\ninternet-connected toys,",
    "start": 224.708,
    "duration": 2.351
  },
  {
    "text": "their online games",
    "start": 227.083,
    "duration": 1.31
  },
  {
    "text": "and many, many, many,\nmany other technologies.",
    "start": 228.417,
    "duration": 2.666
  },
  {
    "text": "So during my research,",
    "start": 232.25,
    "duration": 1.643
  },
  {
    "text": "a lot of parents came up to me\nand they were like, \"So what?",
    "start": 233.917,
    "duration": 4.142
  },
  {
    "text": "Why does it matter\nif my children are being tracked?",
    "start": 238.083,
    "duration": 2.917
  },
  {
    "text": "We've got nothing to hide.\"",
    "start": 242.042,
    "duration": 1.333
  },
  {
    "text": "Well, it matters.",
    "start": 244.958,
    "duration": 1.5
  },
  {
    "text": "It matters because today individuals\nare not only being tracked,",
    "start": 247.083,
    "duration": 6.018
  },
  {
    "text": "they're also being profiled\non the basis of their data traces.",
    "start": 253.125,
    "duration": 4.101
  },
  {
    "text": "Artificial intelligence and predictive\nanalytics are being used",
    "start": 257.25,
    "duration": 3.809
  },
  {
    "text": "to harness as much data as possible\nof an individual life",
    "start": 261.083,
    "duration": 3.643
  },
  {
    "text": "from different sources:",
    "start": 264.75,
    "duration": 1.851
  },
  {
    "text": "family history, purchasing habits,\nsocial media comments.",
    "start": 266.625,
    "duration": 4.518
  },
  {
    "text": "And then they bring this data together",
    "start": 271.167,
    "duration": 1.851
  },
  {
    "text": "to make data-driven decisions\nabout the individual.",
    "start": 273.042,
    "duration": 2.75
  },
  {
    "text": "And these technologies\nare used everywhere.",
    "start": 276.792,
    "duration": 3.434
  },
  {
    "text": "Banks use them to decide loans.",
    "start": 280.25,
    "duration": 2.393
  },
  {
    "text": "Insurance uses them to decide premiums.",
    "start": 282.667,
    "duration": 2.375
  },
  {
    "text": "Recruiters and employers use them",
    "start": 286.208,
    "duration": 2.476
  },
  {
    "text": "to decide whether one\nis a good fit for a job or not.",
    "start": 288.708,
    "duration": 2.917
  },
  {
    "text": "Also the police and courts use them",
    "start": 292.75,
    "duration": 3.101
  },
  {
    "text": "to determine whether one\nis a potential criminal",
    "start": 295.875,
    "duration": 3.518
  },
  {
    "text": "or is likely to recommit a crime.",
    "start": 299.417,
    "duration": 2.625
  },
  {
    "text": "We have no knowledge or control",
    "start": 304.458,
    "duration": 4.06
  },
  {
    "text": "over the ways in which those who buy,\nsell and process our data",
    "start": 308.542,
    "duration": 3.642
  },
  {
    "text": "are profiling us and our children.",
    "start": 312.208,
    "duration": 2.709
  },
  {
    "text": "But these profiles can come to impact\nour rights in significant ways.",
    "start": 315.625,
    "duration": 4.042
  },
  {
    "text": "To give you an example,",
    "start": 320.917,
    "duration": 2.208
  },
  {
    "text": "in 2018 the \"New York Times\"\npublished the news",
    "start": 325.792,
    "duration": 4.059
  },
  {
    "text": "that the data that had been gathered",
    "start": 329.875,
    "duration": 1.976
  },
  {
    "text": "through online\ncollege-planning services --",
    "start": 331.875,
    "duration": 3.059
  },
  {
    "text": "that are actually completed by millions\nof high school kids across the US",
    "start": 334.958,
    "duration": 4.726
  },
  {
    "text": "who are looking for a college\nprogram or a scholarship --",
    "start": 339.708,
    "duration": 3.643
  },
  {
    "text": "had been sold to educational data brokers.",
    "start": 343.375,
    "duration": 3.042
  },
  {
    "text": "Now, researchers at Fordham\nwho studied educational data brokers",
    "start": 347.792,
    "duration": 5.434
  },
  {
    "text": "revealed that these companies\nprofiled kids as young as two",
    "start": 353.25,
    "duration": 5.226
  },
  {
    "text": "on the basis of different categories:",
    "start": 358.5,
    "duration": 3.059
  },
  {
    "text": "ethnicity, religion, affluence,",
    "start": 361.583,
    "duration": 4.185
  },
  {
    "text": "social awkwardness",
    "start": 365.792,
    "duration": 2.059
  },
  {
    "text": "and many other random categories.",
    "start": 367.875,
    "duration": 2.934
  },
  {
    "text": "And then they sell these profiles\ntogether with the name of the kid,",
    "start": 370.833,
    "duration": 5.018
  },
  {
    "text": "their home address and the contact details",
    "start": 375.875,
    "duration": 2.809
  },
  {
    "text": "to different companies,",
    "start": 378.708,
    "duration": 1.851
  },
  {
    "text": "including trade and career institutions,",
    "start": 380.583,
    "duration": 2.459
  },
  {
    "text": "student loans",
    "start": 384.083,
    "duration": 1.268
  },
  {
    "text": "and student credit card companies.",
    "start": 385.375,
    "duration": 1.75
  },
  {
    "text": "To push the boundaries,",
    "start": 388.542,
    "duration": 1.351
  },
  {
    "text": "the researchers at Fordham\nasked an educational data broker",
    "start": 389.917,
    "duration": 3.809
  },
  {
    "text": "to provide them with a list\nof 14-to-15-year-old girls",
    "start": 393.75,
    "duration": 5.809
  },
  {
    "text": "who were interested\nin family planning services.",
    "start": 399.583,
    "duration": 3.375
  },
  {
    "text": "The data broker agreed\nto provide them the list.",
    "start": 404.208,
    "duration": 2.476
  },
  {
    "text": "So imagine how intimate\nand how intrusive that is for our kids.",
    "start": 406.708,
    "duration": 4.875
  },
  {
    "text": "But educational data brokers\nare really just an example.",
    "start": 412.833,
    "duration": 3.976
  },
  {
    "text": "The truth is that our children are being\nprofiled in ways that we cannot control",
    "start": 416.833,
    "duration": 4.685
  },
  {
    "text": "but that can significantly impact\ntheir chances in life.",
    "start": 421.542,
    "duration": 3.416
  },
  {
    "text": "So we need to ask ourselves:",
    "start": 426.167,
    "duration": 3.476
  },
  {
    "text": "can we trust these technologies\nwhen it comes to profiling our children?",
    "start": 429.667,
    "duration": 4.684
  },
  {
    "text": "Can we?",
    "start": 434.375,
    "duration": 1.25
  },
  {
    "text": "My answer is no.",
    "start": 437.708,
    "duration": 1.25
  },
  {
    "text": "As an anthropologist,",
    "start": 439.792,
    "duration": 1.267
  },
  {
    "text": "I believe that artificial intelligence\nand predictive analytics can be great",
    "start": 441.083,
    "duration": 3.768
  },
  {
    "text": "to predict the course of a disease",
    "start": 444.875,
    "duration": 2.018
  },
  {
    "text": "or to fight climate change.",
    "start": 446.917,
    "duration": 1.833
  },
  {
    "text": "But we need to abandon the belief",
    "start": 450.0,
    "duration": 1.643
  },
  {
    "text": "that these technologies\ncan objectively profile humans",
    "start": 451.667,
    "duration": 3.684
  },
  {
    "text": "and that we can rely on them\nto make data-driven decisions",
    "start": 455.375,
    "duration": 3.184
  },
  {
    "text": "about individual lives.",
    "start": 458.583,
    "duration": 1.893
  },
  {
    "text": "Because they can't profile humans.",
    "start": 460.5,
    "duration": 2.559
  },
  {
    "text": "Data traces are not\nthe mirror of who we are.",
    "start": 463.083,
    "duration": 3.351
  },
  {
    "text": "Humans think one thing\nand say the opposite,",
    "start": 466.458,
    "duration": 2.101
  },
  {
    "text": "feel one way and act differently.",
    "start": 468.583,
    "duration": 2.435
  },
  {
    "text": "Algorithmic predictions\nor our digital practices",
    "start": 471.042,
    "duration": 2.476
  },
  {
    "text": "cannot account for the unpredictability\nand complexity of human experience.",
    "start": 473.542,
    "duration": 5.166
  },
  {
    "text": "But on top of that,",
    "start": 480.417,
    "duration": 1.559
  },
  {
    "text": "these technologies are always --",
    "start": 482.0,
    "duration": 2.684
  },
  {
    "text": "always --",
    "start": 484.708,
    "duration": 1.268
  },
  {
    "text": "in one way or another, biased.",
    "start": 486.0,
    "duration": 1.917
  },
  {
    "text": "You see, algorithms are by definition\nsets of rules or steps",
    "start": 489.125,
    "duration": 5.059
  },
  {
    "text": "that have been designed to achieve\na specific result, OK?",
    "start": 494.208,
    "duration": 3.709
  },
  {
    "text": "But these sets of rules or steps\ncannot be objective,",
    "start": 498.833,
    "duration": 2.726
  },
  {
    "text": "because they've been designed\nby human beings",
    "start": 501.583,
    "duration": 2.143
  },
  {
    "text": "within a specific cultural context",
    "start": 503.75,
    "duration": 1.726
  },
  {
    "text": "and are shaped\nby specific cultural values.",
    "start": 505.5,
    "duration": 2.5
  },
  {
    "text": "So when machines learn,",
    "start": 508.667,
    "duration": 1.726
  },
  {
    "text": "they learn from biased algorithms,",
    "start": 510.417,
    "duration": 2.25
  },
  {
    "text": "and they often learn\nfrom biased databases as well.",
    "start": 513.625,
    "duration": 3.208
  },
  {
    "text": "At the moment, we're seeing\nthe first examples of algorithmic bias.",
    "start": 517.833,
    "duration": 3.726
  },
  {
    "text": "And some of these examples\nare frankly terrifying.",
    "start": 521.583,
    "duration": 3.5
  },
  {
    "text": "This year, the AI Now Institute\nin New York published a report",
    "start": 526.5,
    "duration": 4.059
  },
  {
    "text": "that revealed that the AI technologies",
    "start": 530.583,
    "duration": 2.393
  },
  {
    "text": "that are being used\nfor predictive policing",
    "start": 533.0,
    "duration": 3.476
  },
  {
    "text": "have been trained on \"dirty\" data.",
    "start": 536.5,
    "duration": 3.125
  },
  {
    "text": "This is basically data\nthat had been gathered",
    "start": 540.333,
    "duration": 2.893
  },
  {
    "text": "during historical periods\nof known racial bias",
    "start": 543.25,
    "duration": 4.184
  },
  {
    "text": "and nontransparent police practices.",
    "start": 547.458,
    "duration": 2.25
  },
  {
    "text": "Because these technologies\nare being trained with dirty data,",
    "start": 550.542,
    "duration": 4.059
  },
  {
    "text": "they're not objective,",
    "start": 554.625,
    "duration": 1.434
  },
  {
    "text": "and their outcomes are only\namplifying and perpetrating",
    "start": 556.083,
    "duration": 4.518
  },
  {
    "text": "police bias and error.",
    "start": 560.625,
    "duration": 1.625
  },
  {
    "text": "So I think we are faced\nwith a fundamental problem",
    "start": 565.167,
    "duration": 3.142
  },
  {
    "text": "in our society.",
    "start": 568.333,
    "duration": 1.643
  },
  {
    "text": "We are starting to trust technologies\nwhen it comes to profiling human beings.",
    "start": 570.0,
    "duration": 4.792
  },
  {
    "text": "We know that in profiling humans,",
    "start": 575.75,
    "duration": 2.518
  },
  {
    "text": "these technologies\nare always going to be biased",
    "start": 578.292,
    "duration": 2.809
  },
  {
    "text": "and are never really going to be accurate.",
    "start": 581.125,
    "duration": 2.726
  },
  {
    "text": "So what we need now\nis actually political solution.",
    "start": 583.875,
    "duration": 2.934
  },
  {
    "text": "We need governments to recognize\nthat our data rights are our human rights.",
    "start": 586.833,
    "duration": 4.709
  },
  {
    "text": "(Applause and cheers)",
    "start": 592.292,
    "duration": 4.083
  },
  {
    "text": "Until this happens, we cannot hope\nfor a more just future.",
    "start": 599.833,
    "duration": 4.084
  },
  {
    "text": "I worry that my daughters\nare going to be exposed",
    "start": 604.75,
    "duration": 2.726
  },
  {
    "text": "to all sorts of algorithmic\ndiscrimination and error.",
    "start": 607.5,
    "duration": 3.726
  },
  {
    "text": "You see the difference\nbetween me and my daughters",
    "start": 611.25,
    "duration": 2.393
  },
  {
    "text": "is that there's no public record\nout there of my childhood.",
    "start": 613.667,
    "duration": 3.184
  },
  {
    "text": "There's certainly no database\nof all the stupid things that I've done",
    "start": 616.875,
    "duration": 4.018
  },
  {
    "text": "and thought when I was a teenager.",
    "start": 620.917,
    "duration": 2.142
  },
  {
    "text": "(Laughter)",
    "start": 623.083,
    "duration": 1.5
  },
  {
    "text": "But for my daughters\nthis may be different.",
    "start": 625.833,
    "duration": 2.75
  },
  {
    "text": "The data that is being collected\nfrom them today",
    "start": 629.292,
    "duration": 3.184
  },
  {
    "text": "may be used to judge them in the future",
    "start": 632.5,
    "duration": 3.809
  },
  {
    "text": "and can come to prevent\ntheir hopes and dreams.",
    "start": 636.333,
    "duration": 2.959
  },
  {
    "text": "I think that's it's time.",
    "start": 640.583,
    "duration": 1.518
  },
  {
    "text": "It's time that we all step up.",
    "start": 642.125,
    "duration": 1.434
  },
  {
    "text": "It's time that we start working together",
    "start": 643.583,
    "duration": 2.476
  },
  {
    "text": "as individuals,",
    "start": 646.083,
    "duration": 1.435
  },
  {
    "text": "as organizations and as institutions,",
    "start": 647.542,
    "duration": 2.517
  },
  {
    "text": "and that we demand\ngreater data justice for us",
    "start": 650.083,
    "duration": 3.101
  },
  {
    "text": "and for our children",
    "start": 653.208,
    "duration": 1.393
  },
  {
    "text": "before it's too late.",
    "start": 654.625,
    "duration": 1.518
  },
  {
    "text": "Thank you.",
    "start": 656.167,
    "duration": 1.267
  },
  {
    "text": "(Applause)",
    "start": 657.458,
    "duration": 1.417
  }
]